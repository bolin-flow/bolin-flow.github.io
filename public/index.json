[{"content":" ","date":null,"permalink":"/exploring/","section":"Exploring","summary":" ","title":"Exploring"},{"content":"","date":null,"permalink":"/tags/model-risk/","section":"Tags","summary":"","title":"Model Risk"},{"content":"The Federal Reserve and OCC\u0026rsquo;s guidance on model risk management outlines standards that banks need to follow to ensure the reliability and integrity of their financial models. Effective model risk management is crucial for maintaining the bank\u0026rsquo;s financial stability and reputation.\nReviewing the Federal Reserve\u0026rsquo;s Supervisory Letter SR 11-7 \u0026ldquo;Guidance on Model Risk Management,\u0026rdquo; can provide more details in the key aspects of an effective model risk management framework, including robust model development, implementation, and use; effective validation; and sound governance, policies, and controls.\nThe use of models invariably presents model risk, which is the potential for adverse consequences from decisions based on incorrect or misused model outputs and reports. Model risk can lead to financial loss, poor business and strategic decision-making, or damage to a banking organization’s reputation.\nModel risk occurs primarily for two reasons:\n(1) a model may have fundamental errors and produce inaccurate outputs when viewed against its design objective and intended business uses; (2) a model may be used incorrectly or inappropriately or there may be a misunderstanding about its limitations and assumptions. Fundamental Errors: A model might have basic flaws, causing it to produce inaccurate results for its intended purpose. Misuse: A model might be used inappropriately, or people might misunderstand its assumptions and limitations.\nModel risk increases with greater model complexity, higher uncertainty about inputs and assumptions, broader extent of use, and larger potential impact. We need effective challenge as a principle for managing model risk, requiring that models undergo critical analysis by people who are objective, knowledgeable, and capable of understanding the model\u0026rsquo;s limitations.\nModel Validation #Model validation is the set of processes and activities intended to verify that models are performing as expected, in line with their design objectives and business uses. Effective validation helps to ensure that models are sound, identifying potential limitations and assumptions and assessing their possible impact. All model components—inputs, processing, outputs, and reports—should be subject to validation; this applies equally to models developed in-house and to those purchased from or developed by vendors or consultants.\nValidation involves a degree of independence from model development and use. Generally, validation is done by staff who are not responsible for model development or use and do not have a stake in whether a model is determined to be valid. As a practical matter, some validation work may be most effectively done by model developers and users; it is essential, however, that such validation work be subject to critical review by an independent party, who should conduct additional activities to ensure proper validation. Overall, the quality of the validation process is indicated by critical review by objective, knowledgeable parties and the actions taken to address issues identified by those parties.\n","date":"10 November 2024","permalink":"/exploring/model_risk_management/","section":"Exploring","summary":"The Federal Reserve and OCC\u0026rsquo;s guidance on model risk management outlines standards that banks need to follow to ensure the reliability and integrity of their financial models.","title":"Model Risk Management for Banking organizations"},{"content":"","date":null,"permalink":"/tags/risk-management/","section":"Tags","summary":"","title":"Risk Management"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":" ","date":null,"permalink":"/developing/","section":"Developing","summary":" ","title":"Developing"},{"content":"","date":null,"permalink":"/tags/django/","section":"Tags","summary":"","title":"Django"},{"content":"Download Django and use it to build a web application with chat functionality. Start by creating a folder named my_project by running the command django-admin startproject my_project. Once we run this command, Django will generate a new folder named my_project, setting up the essential files and structure needed to begin development.\nIn a Django project, we can have multiple apps, each with a specific purpose. These apps should be related to each other in a way that supports the overall functionality of the project. Type python manage.py startapp blog to build one app called blog.\nA folder named blog is created, containing files like admin.py, apps.py, models.py, views.py, and more.\nadmin.py: Manages the integration with Django’s admin interface, allowing for authentication and administrative functions. models.py: A crucial file that defines data models for the app, such as users, comments, and other entities that need to be stored in the database. views.py: Contains functions that control what users see by rendering templates and handling logic for different pages. Migrations: These files handle database changes, allowing us to create and modify database structures as the models evolve. To run the Django project, execute python manage.py runserver in the terminal:\nCreate urls for new app #Create urls.py in the blog App Directory. Remember to import views from the views.py in the current directory. The name of the first url is \u0026lsquo;index\u0026rsquo;. The first parameter, '', is an empty string, meaning this pattern matches the \u0026ldquo;root\u0026rdquo; URL for the app.\nviews.index refers to a view function named index in the views.py file of the current blog app. The index function defines what should be displayed when a user visits the app\u0026rsquo;s root URL. For example, it might return an HTML template or JSON data.\nThe name parameter or name='index' is a unique name for this URL pattern. It\u0026rsquo;s used to refer to this path in templates and other parts of the code. We can use this name to generate URLs within Django templates like {% url 'index' %}.\n# blog/urls.py from django.urls import path from . import views urlpatterns = [ path(\u0026#39;\u0026#39;, views.index, name=\u0026#39;index\u0026#39;), ] Then we need to tell our project or main app that we have this new url in urls.py of app blog. Open urls.py file in main project folder my_project. If we set this app\u0026rsquo;s URL pattern as path('blog/', include('blog.urls')) in the main urls.py, then this empty string\u0026rsquo;\u0026rsquo; will match yourwebsite.com/blog/.\n# my_project/urls.py from django.contrib import admin from django.urls import path, include urlpatterns = [ path(\u0026#39;admin/\u0026#39;, admin.site.urls), path(\u0026#39;blog/\u0026#39;, include(\u0026#39;blog.urls\u0026#39;)) # add path tp the main urls.py, include blog app urls.py ] Build view function for new url #If we open views.py in blog folder, there is nothing here now. We want to create this index function so it can be called in blog/urls.py file.\n# blog/views.py def index(request): # use HttpResponse to return something first return HttpResponse(\u0026#34;This is my first url\u0026#34;) The first url can be found in my_project/urls.py and blog/urls.py. The index function built in blog/views.py is called in blog/urls.py.View the display on http://127.0.0.1:8000/blog/: To define a new URL in Django, follow these steps:\nAdd a URL in the Main urls.py: in my_project/urls.py, add a new URL pattern inside the urlpatterns list. Whenever a new app is created, you should also include its URLs here to make them accessible in the main project. Define URLs in the App\u0026rsquo;s urls.py: Go to the urls.py file in the specific app folder.create one or more URL patterns within the urlpatterns list specifically for that app. If the path string is empty (\u0026rsquo;\u0026rsquo;), it represents the root URL for that app. If the path string is set to something like \u0026lsquo;specific\u0026rsquo;, you can access it through root_path/specific/. Create a view function in app_folder/views.py to handle requests for the new page associated with the newly created URL. This function can return a string, number, list, or other data types. Build view functions for displaying #If we want to create multple number inserted url paths, we can create one path for article ids like \u0026lt;int:article_id\u0026gt; for passing one integer in the url, and then in views.py we define the article function with article_id parameter. Leverage HTML and CSS #In the blog app folder, create a folder named templates, and within it, create another folder named blog. Inside this blog folder, add the first HTML file, index.html. We can render this HTML view in views.py of the blog app by using render(request, 'blog/index.html', {'article_id': article_id}).\ndef article(request, article_id): return render(request, \u0026#39;blog/index.html\u0026#39;, {\u0026#39;article_id\u0026#39;: article_id}) request: the current HTTP request. \u0026lsquo;blog/index.html\u0026rsquo;: the path to the HTML template. {\u0026lsquo;article_id\u0026rsquo;: article_id}: a context dictionary that passes article_id to the template. Afterwards, it\u0026rsquo;s important to add in element blog.apps.BlogConfig in INSTALLED_APPS list of the settings.py of the main project folder. So we are able to return html file to users.\nApp Recognition: Adding blog.apps.BlogConfig registers the app with Django, making it aware of this app. Template \u0026amp; Static Files: Django loads templates and static files only from registered apps. Database Models: Django includes models from registered apps during migrations to set up database tables. Next, we’ll add CSS to improve the page\u0026rsquo;s appearance. Start by creating a folder named static in the blog app directory. Inside static, create another folder named blog, and then add a new file called style.css in static/blog. To import this CSS file in index.html:\nUse {% load static %} to load the static folder Below this line we need specify loading static 'blog/style.css': \u0026lt;link rel=\u0026#34;stylesheet\u0026#34;, type=\u0026#34;text/css\u0026#34;, href=\u0026#34;{% static \u0026#39;blog/style.css\u0026#39; %}\u0026#34; /\u0026gt; Now we can add changes to style.css, for example:\nh1 { text-align: center; color:rgb(25, 162, 203); } Note: Each time we make changes to style.css, we may need to restart the server.\nWork on chatbot user interface #In order to make the UI looks better, we need to edit the index.html and style.css files for the user interface. In the chatbot id\n","date":"2 November 2024","permalink":"/developing/django_chatbot/","section":"Developing","summary":"Download Django and use it to build a web application with chat functionality.","title":"Django Web Development Chatbot"},{"content":"","date":null,"permalink":"/tags/web/","section":"Tags","summary":"","title":"Web"},{"content":"","date":null,"permalink":"/tags/app/","section":"Tags","summary":"","title":"App"},{"content":"Kivy is an open-source Python framework used for developing cross-platform applications on operating systems such as Android, iOS, macOS, Windows, and Linux. with a natural user interface (NUI), such as touch applications and multi-touch applications. It supports natural user interfaces (NUIs), including touch and multi-touch applications. KivyMD is another library that enhances the appearance of Kivy applications by providing Material Design components and widgets. This facilitates the development process and saves time.\nInstall both Kivy and KivyMD before building an application. Start by creating a folder named my-project and open it in Visual Studio Code. After opening this project, create a file named mainapp.py.\nCreate main python and kv files #In mainapp.py, first import App from the Kivy library. Then create a class named MainApp to run the application. Note that the class name MainApp need to match the name of the Python script. This class will be used whenever we run the application.\nIn order to make usre MainApp works as we expected, we need to let it extends one parent MDApp class in KivyMD. So Class MainApp(MDApp) will be used for our application.\nfrom kivy.app import App from kivymd.app import MDApp class MainWidget(): pass class MainApp(MDApp): def build(self): # must have one build and return the widget class in main.kv return MainWidget() # return the widget class in main.kv, we also need have one corresponding one this mainapp.py Create a new file with the extension .kv. The name of this .kv file must match the first part or the prefix of the class name we just created to run the application. Therefore, the .kv file should be named main.kv, and this .kv file is used for the user interface, while mainapp.py is responsible for the backend logic.\nAfter creating main.kv and mainapp.py, to enable communication between the backend logic written in Python and the UI, we need to define two functions in the main class MainApp(MDApp). The first function is build, which actually constructs the application, and the second is on_start, which is executed whenever the application starts. These two functions are crucial to the application\u0026rsquo;s lifecycle.\nThe file main.kv will contain many widgets. Our initial step is to create the main widget . In the build function, we will return this widget and simultaneously create the corresponding widget class in mainapp.py for the kv file. For each widget create in kv file, we need to build one corresponding widget class in the python file.\nRun one kivy application #We can call MainApp().run() to run the app, but currently, main.kv contains nothing. We need to import a layout and extend the main widget as MainWidget(BoxLayout). Open the terminal, navigate to the my-project directory, and run python mainapp.py.\nfrom kivy.app import App from kivymd.app import MDApp from kivy.uix.boxlayout import BoxLayout class MainWidget(BoxLayout): pass class MainApp(MDApp): def build(self): return MainWidget() MainApp().run() # run the app Work on user interface using Kivy #After running the App, we will see one empty application, now we can close it up and head on over to themain.kv for its user interface. Make sure click \u0026rsquo;tab\u0026rsquo; for creating one child of a specific appearance.\nTo enhance the main.kv file, we need to add widgets to create a user interface using Kivy. The syntax is often used in Kivy language files (.kv) to define properties and structure for a class\nCreate a Screen widget that forms the base of the app’s interface. Add a NavigationLayout inside the Screen. The NavigationLayout widget in KivyMD is typically used for implementing a drawer navigation layout. ScreenManager is nested within NavigationLayout, which suggests multiple screens will be managed by this layout. Create a custom screen class HomeScreen with an assigned name. Inside it, add a BoxLayout with a vertical orientation organizes the components in a vertical stack. Use MDToolbar with properties title, elevation and left_action_items. It will be used at the top of the screen as an app bar. left_action_items adds a menu icon on the left side of the toolbar, which can open the navigation drawer. Implement MDNavigationDrawer, Add an MDNavigationDrawer at the same level as ScreenManager within NavigationLayout. Assign id nav_drawer so it can be referenced within the left_action_items to toggle the drawer’s visibility. ContentNavigationDrawer widget serves as the content area for MDNavigationDrawer. When the navigation drawer is opened, ContentNavigationDrawer determines what appears inside it, like icons, text, buttons, or links. \u0026lt;MainWidge\u0026gt;: Screen: NavigationLayout: ScreenManager: id: screen_manager HomeScreen: name: \u0026#39;home screen\u0026#39; BoxLayout: orientation: \u0026#39;vertical\u0026#39; MDToolbar: title: \u0026#39;My app\u0026#39; elevation: 10 left_action_items: [[\u0026#39;menu\u0026#39;, lambda x: nav_drawer.toggle_nav_drawer()]] ScrollView: MDNavigationDrawer: id: nav_drawer ContentNavigationDrawer: id: content_drawer screen_manager: screen_manager nav_drawer: nav_drawer Use angular brackets to create a new widget \u0026lt;ContentNavigationDrawer\u0026gt;. It serves as the main container for the navigation drawer\u0026rsquo;s content. AnchorLayout is used to position the avatar image within the ContentNavigationDrawer, anchored to the left. Image dsplays the avatar or logo image at the top of the navigation drawer. ScrollView allows the list of items in the navigation drawer to be scrollable if there are too many items to fit on the screen. MDList contains list items in a Material Design style, commonly used for navigation options in a drawer. OneLineListItem displays individual menu options for navigation within the drawer. \u0026lt;ContentNavigationDrawer\u0026gt; orientation: \u0026#39;vertical\u0026#39; padding: \u0026#39;8dp\u0026#39; spacing: \u0026#39;8dp\u0026#39; AnchorLayout: anchor_x: \u0026#39;left\u0026#39; size_hint_y: None height: avatar.height Image: id: avatar size_hint: None, None size: \u0026#34;50dp\u0026#34;, \u0026#34;50dp\u0026#34; source: \u0026#39;data/logo.png\u0026#39; ScrollView: MDList: OneLineListItem: text: \u0026#39;Home\u0026#39; OneLineListItem: text: \u0026#39;About us\u0026#39; ","date":"30 October 2024","permalink":"/exploring/kivy_learn/","section":"Exploring","summary":"Kivy is an open-source Python framework used for developing cross-platform applications on operating systems such as Android, iOS, macOS, Windows, and Linux.","title":"Explore Kivy for Cross-platform Applications"},{"content":"","date":null,"permalink":"/tags/kivy/","section":"Tags","summary":"","title":"Kivy"},{"content":"","date":null,"permalink":"/tags/information-retrieval/","section":"Tags","summary":"","title":"Information Retrieval"},{"content":"","date":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM"},{"content":"","date":null,"permalink":"/tags/rag/","section":"Tags","summary":"","title":"RAG"},{"content":"Updated Azure functionalities on 2024-Oct-16\n#Azure Cloud Services #I\u0026rsquo;ve always been curious about interacting with books and documents in a way that deepens my understanding. I want to read in a dynamic, interactive way that sparks new insights and encourages deeper contemplation. I wish there were more visuals, thought-provoking questions, and summarized topics or keywords for each section to help readers comprehend and distill what they read. I also want to control over how the content is organized and navigated, allowing me to make notes or understand it more easily—especially when reading something in my second language.\nThere are numerous tools available to build these functionalities. For instance, we can parse and split large documents, generate interactive responses using LLMs and embedding models, and utilize various information retrieval techniques. For each text chunk, we can also extract keywords using pretrained BERT models or apply clustering to derive relevant topics from the data. While these tools and methods exist, one challenge lies in cohesively integrating these functionalities to create a smooth user experience with the application. Moreover, when policy constraints are involved, we must ensure secure hosting and proper evaluation of models and datasets, especially for unsupervised learning algorithms, generative models, and datasets containing PII. In this article, I focus on exploring cloud-based solutions, as they provide hosting, monitoring, and evaluation capabilities within one platform, streamlining the security compliance process under a single provider.\n","date":"18 September 2024","permalink":"/developing/rag_metrics/","section":"Developing","summary":"Updated Azure functionalities on 2024-Oct-16","title":"What Are We Really Measuring in a RAG Chatbot?"},{"content":"Updated Ollama Web UI on 2024-Aug-24\nDownload Ollama and Run Commands #Ollama is a versatile tool that offers a range of language models with detailed specifications. To get started, download Ollama and explore the different models available on the Ollama website. Once downloaded, you can use Ollama commands in your terminal.\nRun ollama to see the available commands: On the llama3.1 download page, you can find a list of llama3.1 models. There are also comparisons made for different types of models.\nLLM Sizes, Types, and Quantizations #In the context of large language models (LLMs):\nChat: Direct conversation with the model, where the character card serves as your prompt. Instruct: A chat between \u0026ldquo;you\u0026rdquo; and the \u0026ldquo;assistant,\u0026rdquo; following the model\u0026rsquo;s prompt format. Chat-Instruct: A conversation where you interact with a character card using the instruct template. For example, \u0026ldquo;You are an AI playing [X character]; respond as the character would.\u0026rdquo; This is then adapted to formats like Alpaca, Wizard, etc. There is no definitive \u0026ldquo;best\u0026rdquo; option, but for factual information, instruct mode is typically more reliable. However, instruct-chat doesn\u0026rsquo;t necessarily improve character portrayal or produce longer responses. One may work better than the other for a particular model and prompt.\nFor more information on model suffixes and quantization in LLMs, visit this page. For performance statistics on different model sizes and quantizations, check this comment.\nLet\u0026rsquo;s select one instruct small model and download it using ollama with this command: ollama run llama3.1:8b-instruct-q5_K_M\nWe can interact with Ollama directly using the terminal. Now we can send questions directly or use the curl command to interact with the LLM using ollama. curl is a command-line tool used to send requests to URLs. http://localhost:11434/api/generate is the URL endpoint of the local server running on the machine (localhost) at port 11434. The /api/generate path indicates that this request is related to generation exposed by ollama.\ncurl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;gemma2:2b-instruct-q4_K_M\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;what is water made of?\u0026#34;, \u0026#34;stream\u0026#34;: false }\u0026#39; | jq . d stands for \u0026ldquo;data\u0026rdquo; we are passing and is used to send the specified data in a POST request. The data contains arguments for ollama (JSON object with three key-value pairs). \u0026ldquo;stream\u0026rdquo; false means instead of streaming back one word at a time, we want everything all at once. The | is a pipe that takes the output of the curl command and passes it to another command. jq is a command-line utility for parsing and processing JSON data. . tells jq to output the entire JSON response, formatted for easy reading. View Modelfile, System, and Template #Run ollama show model_name --modelfile to view the modelfile, relevant description, how the model works, and where it lives.\nollama show gemma2:2b-instruct-q4_K_M --modelfile\nTEMPLATE: Displays the system prompt if it exists. SYSTEM: Describes the chat scenario between a user and an AI assistant. To create a custom system prompt for a selected model, first copy the modelfile into your own file called new-modelfile. Let\u0026rsquo;s say the model is gemma2:2b-instruct-q4_K_M. Use the command ollama show gemma2:2b-instruct-q4_K_M --modelfile \u0026gt; new-modelfile to copy the modelfile to the new file. Then modify and save this new-modelfile file with the updated system prompt. Finally, create a new model from the updated modelfile using ollama create new-gemma2_2b_instruct —-file new-modelfile. Run ollama run gemma2_2b_instruct to interact with the updated model!\nRun LLMs with Ollama #Download the ollama Python package, import ollama, and then view the details with ollama.list()\nView model details in the list. Build Custom System Prompt #To create a new model file from an existing model, you can follow these steps. This process involves copying the model file using the \u0026gt; operator and creating new-modelfile using Ollama.\nStart by copying the modelfile into a new file named new-modelfile. You can do this by running the following command in your terminal: ollama show gemma2:2b-instruct-q4_K_M --modelfile \u0026gt; new-modelfile This command will extract the model file for gemma2:2b-instruct-q4_K_M and save it as new-modelfile in your current directory.\nTo view the contents of the newly created new-modelfile, use the cat command:cat new-modelfile. To confirm where the new-modelfile has been saved, you can use the pwd command to print the current working directory.\nHow to edit modelfile in Visual Studio Code. If you want to edit the model file using Visual Studio Code, start by enabling the command to open files directly from the terminal. Open the Command Palette by pressing `Cmd+Shift+P` (Mac), type Shell Command, and select Install 'code' command in PATH. Once enabled, you can open the new-modelfile by running the command: `code new-modelfile` to open this file. After opening the file, you can add a custom prompt or make other modifications to the downloaded language model. pwd code new-modelfile Run Docker Image for Ollama Web UI #We can create an environment that can execute applications consistently across different systems using Docker. Using Docker for this chatbot UI provides a stable and isolated environment, ensuring the application runs smoothly across multiple systems. It simplifies setup, encapsulates dependencies, and allows for easy deployment with a single command. Docker also enables data persistence, scalability, and enhanced security, making it an ideal choice for managing your chatbot interface efficiently.\nView to code to run a Docker container for Ollama Web UI docker run -d -p 3000:8080 \\ --add-host=host.docker.internal:host-gateway \\ -v ollama-webui:/app/backend/data \\ --name ollama-webui \\ --restart always \\ ghcr.io/ollama-webui/ollama-webui:main docker run: Creates a new container. -d: Runs the container in the background (detached mode). -p 3000:8080: Maps host port 3000 to container port 8080. \u0026ndash;add-host=host.docker.internal:host-gateway: Allows the container to access services on the host machine. -v: Creates a volume (ollama-webui) to persist data between sessions. \u0026ndash;name: Names the container ollama-webui for easy identification. \u0026ndash;restart always: Ensures the container restarts automatically if it stops. Image: ghcr.io/ollama-webui/ollama-webui:main specifies the main branch of the Ollama Web UI container image. After running the above command, if you see the message \u0026ldquo;Unable to find image \u0026lsquo;ghcr.io/ollama-webui/ollama-webui locally, main: Pulling from ollama-webui/ollama-webui,\u0026rdquo; the image will then be downloaded. Once completed, run docker ps to display all running containers.\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4b7938e75eac ghcr.io/ollama-webui/ollama-webui:main \u0026#34;bash start.sh\u0026#34; 2 minutes ago Up 2 minutes 0.0.0.0:3000-\u0026gt;8080/tcp ollama-webui You can now access the web UI at http://127.0.0.1:3000/auth/. After signing up with a mock email address, you can select the locally downloaded model and start asking questions. Enjoy exploring!\nSign up and interact with Ollama Web UI There are other ways to build our own chatbot UI. For example, we can also explore Streamlit to integrate chat history and provide options to select different local models.\n","date":"12 August 2024","permalink":"/developing/own_chatgpt/","section":"Developing","summary":"Updated Ollama Web UI on 2024-Aug-24","title":"Build Own ChatGPT: A Simple How-To"},{"content":"","date":null,"permalink":"/tags/ollama/","section":"Tags","summary":"","title":"Ollama"},{"content":"","date":null,"permalink":"/tags/creative-coding/","section":"Tags","summary":"","title":"Creative Coding"},{"content":"","date":null,"permalink":"/tags/generative-art/","section":"Tags","summary":"","title":"Generative Art"},{"content":"Ye Qianqian\u0026rsquo;s Shan Shui work.\n","date":"11 August 2024","permalink":"/exploring/shanshui_mountains/","section":"Exploring","summary":"Ye Qianqian\u0026rsquo;s Shan Shui work.","title":"Moving Mountains: Shan Shui in Motion"},{"content":" My name is Bolin. I'm documenting my learning notes on this website, where I share my passion for developing statistical solutions, building machine learning models, and exploring interesting ideals. As a principal data scientist at Zinnia, my focus now is on building data-driven products, optimizing generative AI and advancing machine learning applications. Feel free to connect—I’m always excited to explore new perspectives and learn from others. ","date":null,"permalink":"/about/","section":"👋 Hello!","summary":"My name is Bolin.","title":"👋 Hello!"},{"content":"","date":null,"permalink":"/tags/literature-review/","section":"Tags","summary":"","title":"Literature Review"},{"content":"This post primarily discusses the challenges faced by RAG, as explored in the paper, and shares some insights from a practical perspective.\nThe paper addresses the persistent issue of hallucination, where large language models tend to generate false information. These hallucinations pose significant challenges in fields requiring high levels of accuracy and precision, such as healthcare, finance, and law. The critical nature of this problem is underscored by recent court cases where ChatGPT produced citations of non-existent legal rulings. The discussion delves into the ongoing challenges that Retrieval-Augmented Generation (RAG) faces in practice, the inherent complexity of hallucinations, and the implications for deploying RAG LLMs in real-world scenarios.\nExperimental Method # To minimize this issue, we designed an experiment where academics review their own educational background, work experience, and publications using their curriculum vitae. The CV serves as a best-case proxy for search results from a fully-functional RAG system.\nThe experimental method introduces a scenario involving academic CV reviews. In the experiment, CV is assumed to represent the ideal output that a fully-functional RAG system could produce. In this setup, we can eliminate the uncertainties that RAG itself could introduce, allowing for a deeper exploration of the hallucination problem in LLMs.\nExperiment Steps # Collected CVs and responses from academic community members. Subjects then uploaded their CVs along with demographic information, including names and current institutions. This information was used to build the prompts. Once loaded, the subjects were guided through a series of interactions with the LLM. Control Variables: #The subjects were presented with three prompt/response pairs under two different conditions. In each case, the participant\u0026rsquo;s name and institution were used.\nIn Condition 1, the CV was not included as context. While in Condition 2, it was included. Participants were randomly assigned to either the \u0026ldquo;with context\u0026rdquo; or \u0026ldquo;without context\u0026rdquo; setting. All tests were conducted using the OpenAI GPT-3.5-turbo-16k-0613 model, which supports up to 16,000 tokens, suitable for particularly long CVs.\nResults # Out of a total of 1,125 items, 68 were identified as incorrect by the users reviewing the responses. Why are we still unable to capture the correct information?\nNoisy Context and Addtional Content #This is the most common error in context-based response augmentation. GPT3.5 occasionally generates responses using context from one section but continues into the next section instead of stopping after retrieving the relevant information from the prompt. For example, the generated text for the \u0026ldquo;Education\u0026rdquo; section might incorrectly include information from the \u0026ldquo;Work History\u0026rdquo; section.\nIt was also observed that GPT3.5 does not always strictly follow the instructions provided in the user prompt. Sometimes it introduces irrelevant or partially related context, and at other times, it fails to adhere to the specified number of items mentioned in the prompt.\nMismatches, Unusual Formatting and Incomplete Context #In several instances, the prompt’s query and the provided context were not fully aligned for the model to extract an accurate answer. Besides, it was observed that the model did not correctly capture or \u0026ldquo;understand\u0026rdquo; the information in the prompt, sometimes mistaking educational background for work experience.\nThe model \u0026ldquo;creatively\u0026rdquo; filled in the gaps by generating a list of jobs for the empty section. It added fabricated jobs that seemed more credible by drawing on information from the education and publication histories to create a believable sequence. This highlights that the model\u0026rsquo;s \u0026ldquo;hallucinations\u0026rdquo; can be more difficult to detect and that the model\u0026rsquo;s and humans\u0026rsquo; interpretations of certain items might differ.\nDue to the unique formatting of dates in the CV, the model occasionally misassigned dates to the wrong experiences. Some CVs included links, but since the model cannot access external web information, it generated responses that users considered incomplete, as they didn’t provide any details beyond the given context.\nPerspectives #RAG systems with LLMs cannot fully serve as reliable \u0026ldquo;libraries\u0026rdquo; for information retrieval; they still produce varying degrees of hallucinations and errors. RAG (Retrieval-Augmented Generation) has become a popular method to reduce these hallucinations. RAGs dynamically retrieve and incorporate data from external sources based on the user’s query. By indexing similar text chunks, we provide LLMs with supplementary information to help generate more accurate responses. However, it’s important to acknowledge that RAG itself has limitations:\nFirstly, does the text indexed by RAG truly contain relevant content to assist in generating accurate responses? This research has shown that in the presence of complex or misleading search results, a RAG system may often get things wrong. Secondly, trained GPT model constrained by its training data. As mentioned in the paper, Emily Bender’s octopus analogy illustrates this well—where an intelligent octopus (standing in for an LLM) has learned to imitate human communication solely based on statistical patterns in intercepted undersea cable transmissions. LLMs can indeed accelerate development and productization, replacing some basic to intermediate NLP tasks. They are most accurate when operating within domains where the training set contains many correct and largely aligned examples. However, if our data includes conflicting sources or is contaminated with inaccurate information, the models will struggle to produce reliable content. Additionally, we need to keep in mind that the statistical patterns in the training data and a single large language model cannot fully capture the contextual nuances and specialized knowledge required for a specific industry.\nReminders #Given the potential error rate of RAG systems, we need to consider what level of accuracy is acceptable for our real-world scenarios. When dealing with large volumes of data, the scale of this issue becomes more apparent. For example, in 2023, an estimated 8.5 billion Google searches were conducted daily. An equivalent advanced RAG system could potentially generate misleading or incorrect information around 500 million results per day. And this doesn’t even account for additional uncertainties and noisy data introduced in real-world applications.\nTherefore, relying entirely on LLMs would be overly optimistic. LLMs are great foundational tools for generating results, and I think it is essential to combine them with well-defined business rules and customized machine learning algorithms to develop a system that is both easy to monitor and evaluate.\n","date":"16 July 2024","permalink":"/exploring/double_edged_sword_rag/","section":"Exploring","summary":"This post primarily discusses the challenges faced by RAG, as explored in the paper, and shares some insights from a practical perspective.","title":"The Double-Edged Sword of RAG Chatbots"},{"content":" Open source, machine learning, creative coding, and sharing ideas… ","date":null,"permalink":"/","section":"","summary":" Open source, machine learning, creative coding, and sharing ideas… ","title":""},{"content":"","date":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo"},{"content":"There are some methods to integrate visualizations into a self-hosted Hugo website.\nFor a markdown page in the Hugo content folder, start by creating a folder for each post. Inside this post folder, create an index.md file. This is where we\u0026rsquo;ll edit the content for that post. Hugo allows you to organize posts in different ways such as branch pages, leaf pages or other custom layouts, depending on the preferences. I’m using the Congo theme, which comes with some helpful explanations in the documentation.\nThe first simple way to add visualizations is to include images inside the content folder. We can download some illustration by Katerina Limpitsouni.\nDownload svg imagesundraw_heart.svg, undraw_cloud.svg, and undraw_floating.svg. There are different ways to insert images to one markdown file.\n\u0026lt;div style=\u0026#34;display: flex; justify-content: center; align-items: center;\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;undraw_heart.svg\u0026#34; alt=\u0026#34;Heart\u0026#34; style=\u0026#34;margin-left: 10px;\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;undraw_cloud.svg\u0026#34; alt=\u0026#34;Cloud\u0026#34; style=\u0026#34;margin-left: 10px;\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;undraw_heart.svg\u0026#34; alt=\u0026#34;Heart\u0026#34; style=\u0026#34;margin-left: 10px;\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;undraw_cloud.svg\u0026#34; alt=\u0026#34;Cloud\u0026#34; style=\u0026#34;margin-left: 10px;\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;undraw_heart.svg\u0026#34; alt=\u0026#34;Heart\u0026#34; style=\u0026#34;margin-left: 10px;\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;undraw_cloud.svg\u0026#34; alt=\u0026#34;Cloud\u0026#34; style=\u0026#34;margin-left: 10px;\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; We can also use Hugo’s built-in shortcode for a simpler way to include images.\n{{ \u0026lt; figure src=\u0026ldquo;undraw_floating.svg\u0026rdquo; class=\u0026ldquo;m-auto mt-6 max-w-prose\u0026rdquo; \u0026gt; }}\nIllustration by Katerina Limpitsouni Another creative approach is to embed interactive visualizations using the p5.js JavaScript library. The main page of my website features a creative coding effect built by oggy\u0026rsquo;s artwork. I made several modifications to adapt it to different window sizes, adjust the speed and transparency of the triangles, and update the background colors based on user interactions.\nWondering how to add interactive visuals to your own webpages? Start by creating a js folder inside the static folder of your Hugo web repository. We can place various visualizations built with p5.js or other tools in this folder and then call them on specific pages as needed. This approach is inspred by Ariel Mundo\u0026rsquo;s sharing and Deisy Gysi’s main page.\nNow, let’s create a simple interactive particles.js file for the current webpage that responds to left mouse clicks. Below, we break down the code into 8 blocks, explaining how each part contributes to the visualizations triggered by user actions. Try clicking the left mouse button once or twice outside the image area or code block to see the visual effects in action!\nClick to view explanations of the key steps for generating stars and hearts with clicks. Step 1: Setting Up Variables\nlet particles = []; let clickCount = 0; particles is an array that will hold all the particle objects we create. clickCount keeps track of the number of left mouse clicks to determine which shape to generate. Step 2: Setting Up the Canvas\nCreates a full-window canvas positioned behind other content.\nfunction setup() { let canvas = createCanvas(windowWidth, windowHeight); canvas.position(0, 0); canvas.style(\u0026#39;z-index\u0026#39;, \u0026#39;-1\u0026#39;); canvas.style(\u0026#39;position\u0026#39;, \u0026#39;fixed\u0026#39;); } setup() is a special function in p5.js that runs once when the program starts. createCanvas(windowWidth, windowHeight) creates a canvas that covers the entire browser window. canvas.position(0, 0) positions the canvas at the top-left corner of the window. canvas.style('z-index', '-1') moves the canvas behind other content on the page. canvas.style('position', 'fixed') ensures the canvas stays fixed in place as the user scrolls. Step 3: Drawing and Updating the Canvas\nfunction draw() { clear(); // Clears the canvas, making it transparent for (let i = particles.length - 1; i \u0026gt;= 0; i--) { particles[i].update(); particles[i].show(); if (particles[i].finished()) { particles.splice(i, 1); } } } Continuously clears and redraws particles, removing them when they fade out.\ndraw() is a special function that continuously executes the code inside it, typically at 60 frames per second. clear() clears the entire canvas, making it transparent, so the previous frame is removed. This for loop goes through each particle in the particles array in reverse order (to avoid issues when removing particles). particles[i].update() updates the particle\u0026rsquo;s position and state. particles[i].show() displays the particle on the canvas. If the particle has \u0026ldquo;finished\u0026rdquo; (its alpha value is below zero), it\u0026rsquo;s removed from the array using particles.splice(i, 1). Step 4: Handling Mouse Clicks\nfunction mousePressed() { clickCount++; setTimeout(() =\u0026gt; { if (clickCount === 1) { addParticles(\u0026#39;star\u0026#39;); } else if (clickCount === 2) { addParticles(\u0026#39;heart\u0026#39;); } clickCount = 0; // Reset click count }, 250); } Tracks single or double clicks to decide which shape to generate.\nmousePressed() is a built-in p5.js function that triggers whenever the mouse is pressed. clickCount++ increments the clickCount variable each time the mouse is clicked. setTimeout() waits for 250 milliseconds to check the number of clicks. If clickCount equals 1, it calls addParticles('star') to generate star-shaped particles. If clickCount equals 2, it calls addParticles('heart') to generate heart-shaped particles. clickCount = 0 resets the click count after processing. Step 5: Adding Particles\nfunction addParticles(shape) { for (let i = 0; i \u0026lt; 10; i++) { // Generate 10 particles particles.push(new Particle(mouseX, mouseY, shape)); } } addParticles(shape) creates 10 new particles at the current mouse position (mouseX, mouseY). Each particle is pushed into the particles array with a specified shape (star or heart). Step 6: Adjusting Canvas Size on Window Resize\nfunction windowResized() { resizeCanvas(windowWidth, windowHeight); } windowResized() is a p5.js function that triggers whenever the browser window is resized. resizeCanvas(windowWidth, windowHeight) adjusts the canvas size to match the new window dimensions. Step 7: Creating the Particle Class\nclass Particle { constructor(x, y, shape) { this.x = x; this.y = y; this.vx = random(-1, 1); this.vy = random(-1, 1); this.alpha = 255; this.color = color(random(255), random(255), random(255), this.alpha); this.shape = shape; } } This code defines a Particle class. The constructor(x, y, shape) method initializes each particle with: x, y: Position on the canvas. vx, vy: Random velocity in both directions. alpha: Initial transparency (255 is fully opaque). color: Random color with the specified alpha. shape: The shape of the particle (star or heart). Step 8: Updating and Showing Star and Heart Particles\nupdate() { this.x += this.vx; this.y += this.vy; this.alpha -= 5; this.color.setAlpha(this.alpha); // Update alpha for fading effect } finished() { return this.alpha \u0026lt; 0; } show() { noStroke(); fill(this.color); if (this.shape === \u0026#39;star\u0026#39;) { beginShape(); for (let i = 0; i \u0026lt; 5; i++) { let angle = TWO_PI / 5 * i; let x = this.x + cos(angle) * 8; let y = this.y + sin(angle) * 8; vertex(x, y); angle += TWO_PI / 10; x = this.x + cos(angle) * 4; y = this.y + sin(angle) * 4; vertex(x, y); } endShape(CLOSE); } else if (this.shape === \u0026#39;heart\u0026#39;) { beginShape(); vertex(this.x, this.y); bezierVertex(this.x - 5, this.y - 5, this.x - 10, this.y + 5, this.x, this.y + 10); bezierVertex(this.x + 10, this.y + 5, this.x + 5, this.y - 5, this.x, this.y); endShape(CLOSE); } } } update(): Updates the particle\u0026rsquo;s position by adding the velocity to x and y. update() also decreases the alpha value, making the particle more transparent over time. finished(): Returns true if the particle is fully transparent (alpha \u0026lt; 0), indicating it should be removed from the particles array. show(): Displays the particle on the canvas: noStroke() removes any outline from the shape. fill(this.color) fills the shape with the random particle\u0026rsquo;s color. If the particle\u0026rsquo;s shape is star, it draws a star using beginShape(), vertex(), and trigonometric functions (cos, sin). If the shape is heart, it draws a heart using bezierVertex() to create smooth curves. ","date":"2 July 2024","permalink":"/developing/interactive_visual/","section":"Developing","summary":"There are some methods to integrate visualizations into a self-hosted Hugo website.","title":"Integrate Interactive Visuals to Hugo Webpage"},{"content":"Derek Sivers Tech Blogs #Learn from Derek Sivers\u0026rsquo;s blogs\nCongo Config Parameters #Congo layout configuration parameters\nWarping or dommain distortion #Warping, or dommain distortion technique in computer graphics\nOggy interactive visual arts #Oggy open processing art works\nLilian Weng blogs and notes #Lilian Weng\u0026rsquo;s blogs, documentations and learning notes\nEmoji Search\nKaterina Limpitsouni and unDraw #Katerina Limpitsouni Open-source project unDraw\nJavaScript animations using p5.js #Adding JavaScript to Hugo web page animations using p5.js\nDeisy Morselli Gysi webpage #Beautiful background in Deisy Gysi’s site\n# ","date":null,"permalink":"/about/useful_links/","section":"👋 Hello!","summary":"Useful links for reviewing or contamplations.","title":"📎 Useful Links"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]